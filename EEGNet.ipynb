{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBuHUHfE3MWg+pB9KHkOSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrzhd/EEGNet/blob/main/EEGNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motor Imagery Task Classification"
      ],
      "metadata": {
        "id": "x_5wKiVyBTzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Data"
      ],
      "metadata": {
        "id": "O-3Lg9veBge0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading BCI Competition IV 2a Dataset"
      ],
      "metadata": {
        "id": "W4JrI64nBl52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6jNZi8rBNjN"
      },
      "outputs": [],
      "source": [
        "!wget https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/cleaned_data/"
      ],
      "metadata": {
        "id": "5DbydQswB27q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/BCICIV_2a_gdf.zip -d raw_data"
      ],
      "metadata": {
        "id": "50hgj12yB54U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Packages"
      ],
      "metadata": {
        "id": "Y_mkff9aB-9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mne"
      ],
      "metadata": {
        "id": "mqHVmwYyCBsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch-summary"
      ],
      "metadata": {
        "id": "ueJFbf8KCCPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries Used"
      ],
      "metadata": {
        "id": "crxgIV9ICD2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "CEp96MytCGei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring Data"
      ],
      "metadata": {
        "id": "nn8IHP2_CQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_folder = '/content/raw_data/'\n",
        "cleaned_data_folder = '/content/cleaned_data/'\n",
        "files = os.listdir(raw_data_folder)\n",
        "\n",
        "# Filtering out files with suffix 'E.gdf'\n",
        "filtered_files = [file for file in files if file.endswith('T.gdf')]\n",
        "\n",
        "raw_list = []\n",
        "\n",
        "# Iterating through filtered files\n",
        "for file in filtered_files:\n",
        "    file_path = os.path.join(raw_data_folder, file)\n",
        "\n",
        "    # Reading raw data\n",
        "    raw = mne.io.read_raw_gdf(file_path, eog=['EOG-left', 'EOG-central', 'EOG-right'], preload=True)\n",
        "    # Droping EOG channels\n",
        "    raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
        "\n",
        "    # High Pass Filtering 4Hz and above\n",
        "    raw.filter(l_freq=4, h_freq=None)\n",
        "\n",
        "    # Saving the modified raw data to a file with .fif suffix\n",
        "    new_file_path = os.path.join(cleaned_data_folder, file[:-4] + '.fif')\n",
        "    raw.save(new_file_path, overwrite=True)\n",
        "    # Appending data to the list\n",
        "    raw_list.append(raw)\n",
        "\n",
        "final_raw = mne.concatenate_raws(raw_list)\n",
        "new_file_path = os.path.join(cleaned_data_folder, 'All_Subjects.fif')\n",
        "final_raw.save(new_file_path, overwrite=True)\n"
      ],
      "metadata": {
        "id": "X6ViqHGaCUWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List of the events**  \n",
        "'1023': 1 Rejected trial  \n",
        "'1072': 2 Eye movements  \n",
        "'276': 3 Idling EEG (eyes open)  \n",
        "'277': 4 Idling EEG (eyes closed)  \n",
        "'32766': 5 Start of a new run  \n",
        "'768': 6 Start of a trial  \n",
        "'769': 7 Cue onset **Left** (class 1) : 0  \n",
        "'770': 8 Cue onset **Right** (class 2) : 1  \n",
        "'771': 9 Cue onset **Foot** (class 3) : 2  \n",
        "'772': 10 Cue onset **Tongue** (class 4): 3"
      ],
      "metadata": {
        "id": "wNqq3V3rCiPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events = mne.events_from_annotations(final_raw)\n",
        "events[1]\n"
      ],
      "metadata": {
        "id": "-9RZjQbWCoc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = mne.Epochs(final_raw, events[0], event_id=[7, 8, 9, 10], tmin=3.75, tmax=5.75, reject=None, baseline=None, preload=True)\n",
        "data = epochs.get_data(copy=True)"
      ],
      "metadata": {
        "id": "OaAuA1mQCqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset's shape:\",data.shape)"
      ],
      "metadata": {
        "id": "rVNRjQbfCs3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Normalizing Labels to [0, 1, 2, 3]\n",
        "labels = epochs.events[:,-1]\n",
        "min_label = np.min(labels)\n",
        "y = labels - min_label\n",
        "\n",
        "X = epochs.get_data(copy=True)\n",
        "\n",
        "# Spliting  Data: 80% for Train and 20% for Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Converting to PyTorch Tensors\n",
        "X_train = X_train.reshape((X_train.shape[0], -1, X_train.shape[-1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1, X_test.shape[-1]))\n",
        "X_train = torch.Tensor(X_train)\n",
        "X_test = torch.Tensor(X_test)\n",
        "X_train = X_train.unsqueeze(1).to(device)\n",
        "X_test = X_test.unsqueeze(1).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "y_test = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "# Printing the sizes\n",
        "print(\"Size of X_train:\", X_train.size())\n",
        "print(\"Size of X_test:\", X_test.size())\n",
        "print(\"Size of y_train:\", y_train.size())\n",
        "print(\"Size of y_test:\", y_test.size())\n"
      ],
      "metadata": {
        "id": "D5dF6ZydC2PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EEGNet Model"
      ],
      "metadata": {
        "id": "vR2z57zqC5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGNet(nn.Module):\n",
        "    def __init__(self,  chans=22, classes=4, kernLength=32, time_points=501,\n",
        "                 f1=8, f2=16, d=2, dropoutRate=0.5, max_norm1=1, max_norm2=0.25):\n",
        "        super(EEGNet, self).__init__()\n",
        "        # Calculating FC input features\n",
        "        linear_input_size = (((time_points)//8)//16)*f2\n",
        "\n",
        "        # Temporal Filters\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, f1, (1, kernLength), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(f1),\n",
        "        )\n",
        "        # Spatial Filters\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(f1, d * f1, (chans, 1), groups=f1, bias=False), # Depthwise Conv\n",
        "            nn.BatchNorm2d(d * f1),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(dropoutRate)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(d * f1, f2, (1, 16), padding='same', bias=False), # Separable Conv\n",
        "            nn.Conv2d(f2, f2, kernel_size=1), # Pointwise Conv\n",
        "            nn.BatchNorm2d(f2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 16)),\n",
        "            nn.Dropout(dropoutRate)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(linear_input_size, classes)\n",
        "\n",
        "        # Apply max_norm constraint to the depthwise layer in block2\n",
        "        self._apply_max_norm(self.block2[0], max_norm1)\n",
        "\n",
        "        # Apply max_norm constraint to the linear layer\n",
        "        self._apply_max_norm(self.fc, max_norm2)\n",
        "\n",
        "    def _apply_max_norm(self, layer, max_norm):\n",
        "        for name, param in layer.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                param.data = torch.renorm(param.data, p=2, dim=0, maxnorm=max_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SmFhrHzSC8BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Summery"
      ],
      "metadata": {
        "id": "M49ks2b8C_dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (1, 22, 501)\n",
        "eegnet_model = EEGNet().to(device)\n",
        "summary(eegnet_model, input_size)"
      ],
      "metadata": {
        "id": "7NdOeIRtDAOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ],
      "metadata": {
        "id": "95r4EymoDCIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eegnet_model = EEGNet().to(device)\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(eegnet_model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 500\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    eegnet_model.train()\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        inputs = X_train[i:i+batch_size].to(device)\n",
        "        labels = y_train[i:i+batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = eegnet_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(X_train)\n",
        "    epoch_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {(epoch_accuracy*100):.2f}%\")\n",
        "average_loss = running_loss / len(X_train)\n",
        "print(\"Average Loss:\", average_loss)\n",
        "\n",
        "# Saving model\n",
        "torch.save(eegnet_model, 'eegnet_model.pth')\n"
      ],
      "metadata": {
        "id": "TJzl6On9DFcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing Model"
      ],
      "metadata": {
        "id": "-3fKnNHxDKwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eegnet_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i in range(len(X_test)):\n",
        "        inputs = X_test[i:i+1].to(device)\n",
        "        labels = y_test[i:i+1].to(device)\n",
        "        outputs = eegnet_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = (correct / total)*100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "aHsstvZSDLb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confusion Matrix"
      ],
      "metadata": {
        "id": "6H1V2tn-DNkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eegnet_model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "classes = ['Left', 'Right', 'Foot', 'Tongue']\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in zip(X_test, y_test):\n",
        "        outputs = eegnet_model(inputs.unsqueeze(0))  # Forward pass\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_pred.append(predicted.item())\n",
        "        y_true.append(labels.item())\n",
        "\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "cf_matrix = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='.2f')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('confusion_matrix_eegnet.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "egFgd6LxDPko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}