{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6G3YhFDXBPVr7DCT5NLl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrzhd/EEGNet/blob/main/EEGNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motor Imagery Task Classification"
      ],
      "metadata": {
        "id": "x_5wKiVyBTzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Data"
      ],
      "metadata": {
        "id": "O-3Lg9veBge0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading BCI Competition IV 2a Dataset"
      ],
      "metadata": {
        "id": "W4JrI64nBl52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6jNZi8rBNjN"
      },
      "outputs": [],
      "source": [
        "!wget https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/cleaned_data/"
      ],
      "metadata": {
        "id": "5DbydQswB27q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/BCICIV_2a_gdf.zip -d raw_data"
      ],
      "metadata": {
        "id": "50hgj12yB54U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Packages"
      ],
      "metadata": {
        "id": "Y_mkff9aB-9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mne"
      ],
      "metadata": {
        "id": "mqHVmwYyCBsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch-summary"
      ],
      "metadata": {
        "id": "ueJFbf8KCCPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries Used"
      ],
      "metadata": {
        "id": "crxgIV9ICD2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "CEp96MytCGei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring Data"
      ],
      "metadata": {
        "id": "nn8IHP2_CQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_folder = '/content/raw_data/'\n",
        "cleaned_data_folder = '/content/cleaned_data/'\n",
        "files = os.listdir(raw_data_folder)\n",
        "\n",
        "# Filtering out files with suffix 'E.gdf'\n",
        "filtered_files = [file for file in files if file.endswith('T.gdf')]\n",
        "\n",
        "raw_list = []\n",
        "\n",
        "# Iterating through filtered files\n",
        "for file in filtered_files:\n",
        "    file_path = os.path.join(raw_data_folder, file)\n",
        "\n",
        "    # Reading raw data\n",
        "    raw = mne.io.read_raw_gdf(file_path, eog=['EOG-left', 'EOG-central', 'EOG-right'], preload=True)\n",
        "    # Droping EOG channels\n",
        "    raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
        "\n",
        "    # High Pass Filtering 4-40 Hz\n",
        "    raw.filter(l_freq=4, h_freq=40, method='iir')\n",
        "\n",
        "    # Notch filter for Removal of Line Voltage\n",
        "    raw.notch_filter(freqs=50)\n",
        "\n",
        "    # Resampling Data\n",
        "    raw.resample(128, npad='auto')\n",
        "\n",
        "    # Saving the modified raw data to a file with .fif suffix\n",
        "    new_file_path = os.path.join(cleaned_data_folder, file[:-4] + '.fif')\n",
        "    raw.save(new_file_path, overwrite=True)\n",
        "    # Appending data to the list\n",
        "    raw_list.append(raw)\n",
        "\n",
        "final_raw = mne.concatenate_raws(raw_list)\n",
        "new_file_path = os.path.join(cleaned_data_folder, 'All_Subjects.fif')\n",
        "final_raw.save(new_file_path, overwrite=True)\n"
      ],
      "metadata": {
        "id": "X6ViqHGaCUWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List of the events**  \n",
        "'1023': 1 Rejected trial  \n",
        "'1072': 2 Eye movements  \n",
        "'276': 3 Idling EEG (eyes open)  \n",
        "'277': 4 Idling EEG (eyes closed)  \n",
        "'32766': 5 Start of a new run  \n",
        "'768': 6 Start of a trial  \n",
        "'769': 7 Cue onset **Left** (class 1) : 0  \n",
        "'770': 8 Cue onset **Right** (class 2) : 1  \n",
        "'771': 9 Cue onset **Foot** (class 3) : 2  \n",
        "'772': 10 Cue onset **Tongue** (class 4): 3"
      ],
      "metadata": {
        "id": "wNqq3V3rCiPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events = mne.events_from_annotations(final_raw)\n",
        "events[1]\n"
      ],
      "metadata": {
        "id": "-9RZjQbWCoc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = mne.Epochs(final_raw, events[0], event_id=[7, 8, 9, 10], tmin=3.75, tmax=5.75, reject=None, baseline=None, preload=True)\n",
        "data = epochs.get_data(copy=True)"
      ],
      "metadata": {
        "id": "OaAuA1mQCqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset's shape:\",data.shape)"
      ],
      "metadata": {
        "id": "rVNRjQbfCs3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Normalizing Labels to [0, 1, 2, 3]\n",
        "labels = epochs.events[:,-1]\n",
        "y = labels - np.min(labels)\n",
        "\n",
        "# Normalizing Input features: z-score(mean=0, std=1)\n",
        "X = data\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Spliting  Data: 80% for Train and 20% for Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Converting to Tensor\n",
        "X_train = torch.Tensor(X_train).unsqueeze(1).to(device)\n",
        "X_test = torch.Tensor(X_test).unsqueeze(1).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "y_test = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "# Printing the sizes\n",
        "print(\"Size of X_train:\", X_train.size())\n",
        "print(\"Size of X_test:\", X_test.size())\n",
        "print(\"Size of y_train:\", y_train.size())\n",
        "print(\"Size of y_test:\", y_test.size())\n",
        "\n",
        "# Datasets & Data loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader= DataLoader(train_dataset, batch_size= 128, shuffle=True)\n",
        "test_loader= DataLoader(test_dataset, batch_size= 1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "D5dF6ZydC2PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Loader"
      ],
      "metadata": {
        "id": "e96rtoheZ1H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This section is only for K-Fold Cross Validation\n",
        "\"\"\"\n",
        "X_torch = torch.Tensor(X).unsqueeze(1).to(device)\n",
        "y_torch = torch.LongTensor(y).to(device)\n",
        "\n",
        "dataset = TensorDataset(X_torch, y_torch)\n",
        "print(f\"X type:{type(X_torch)}, size: {X_torch.size()}\")\n",
        "print(f\"y type:{type(y_torch)}, size: {y_torch.size()}\")\n"
      ],
      "metadata": {
        "id": "_Te3VWMpZ3FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EEGNet Model"
      ],
      "metadata": {
        "id": "vR2z57zqC5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGNet(nn.Module): # EEGNET-8,2\n",
        "    def __init__(self,  chans=22, classes=4, kernLength=32, time_points=257,\n",
        "                 f1=8, f2=16, d=2, dropoutRate=0.5, max_norm1=1, max_norm2=0.25):\n",
        "        super(EEGNet, self).__init__()\n",
        "        # Calculating FC input features\n",
        "        linear_input_size = (((time_points)//4)//8)*f2\n",
        "\n",
        "        # Temporal Filters\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, f1, (1, kernLength), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(f1),\n",
        "        )\n",
        "        # Spatial Filters\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(f1, d * f1, (chans, 1), groups=f1, bias=False), # Depthwise Conv\n",
        "            nn.BatchNorm2d(d * f1),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(dropoutRate)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(d * f1, f2, kernel_size=(1, 16),  groups=f2, bias=False, padding='same'), # Separable Conv\n",
        "            nn.Conv2d(f2, f2, kernel_size=1, bias=False), # Pointwise Conv\n",
        "            nn.BatchNorm2d(f2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(dropoutRate)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(linear_input_size, classes)\n",
        "\n",
        "        # Apply max_norm constraint to the depthwise layer in block2\n",
        "        self._apply_max_norm(self.block2[0], max_norm1)\n",
        "\n",
        "        # Apply max_norm constraint to the linear layer\n",
        "        self._apply_max_norm(self.fc, max_norm2)\n",
        "\n",
        "    def _apply_max_norm(self, layer, max_norm):\n",
        "        for name, param in layer.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                param.data = torch.renorm(param.data, p=2, dim=0, maxnorm=max_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SmFhrHzSC8BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Summery"
      ],
      "metadata": {
        "id": "M49ks2b8C_dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (1, 22, 257) #1716 Trainable Parameters (As mentioned in paper)\n",
        "eegnet_model = EEGNet().to(device)\n",
        "summary(eegnet_model, input_size)"
      ],
      "metadata": {
        "id": "7NdOeIRtDAOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ],
      "metadata": {
        "id": "95r4EymoDCIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eegnet_model = EEGNet().to(device)\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(eegnet_model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 500\n",
        "batch_size = 64\n",
        "for epoch in range(num_epochs):\n",
        "    eegnet_model.train()\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        inputs = X_train[i:i+batch_size].to(device)\n",
        "        labels = y_train[i:i+batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = eegnet_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(X_train)\n",
        "    epoch_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {(epoch_accuracy*100):.2f}%\")\n",
        "average_loss = running_loss / len(X_train)\n",
        "print(\"Average Loss:\", average_loss)\n",
        "\n",
        "# Saving model\n",
        "torch.save(eegnet_model, 'eegnet_model.pth')\n"
      ],
      "metadata": {
        "id": "TJzl6On9DFcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing Model"
      ],
      "metadata": {
        "id": "-3fKnNHxDKwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eegnet_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i in range(len(X_test)):\n",
        "        inputs = X_test[i:i+1].to(device)\n",
        "        labels = y_test[i:i+1].to(device)\n",
        "        outputs = eegnet_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = (correct / total)*100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "aHsstvZSDLb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confusion Matrix"
      ],
      "metadata": {
        "id": "6H1V2tn-DNkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eegnet_model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "classes = ['Left', 'Right', 'Foot', 'Tongue']\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in zip(X_test, y_test):\n",
        "        outputs = eegnet_model(inputs.unsqueeze(0))  # Forward pass\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_pred.append(predicted.item())\n",
        "        y_true.append(labels.item())\n",
        "\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "cf_matrix = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='.2f')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('confusion_matrix_eegnet.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "egFgd6LxDPko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##K-Fold Cross Validation"
      ],
      "metadata": {
        "id": "JIV324bQaDMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_folds = 4\n",
        "results = {}\n",
        "torch.manual_seed(42)\n",
        "num_epochs = 500\n",
        "batch_size = 8\n",
        "learning_rate = 0.0001\n",
        "\n",
        "def reset_weights(m):\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()\n",
        "\n",
        "# K-fold Cross Validator\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "fold_accuracies = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(X), 1):  # Assuming X contains your data\n",
        "\n",
        "    print(f\"FOLD ({fold})\")\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    # Data loaders for training and testing data in this fold\n",
        "    X_train_fold, X_test_fold = X[train_ids], X[test_ids]\n",
        "    y_train_fold, y_test_fold = y[train_ids], y[test_ids]\n",
        "    train_idx = train_ids[:train_size]\n",
        "    val_idx = train_ids[train_size:train_size + val_size]\n",
        "    test_idx = test_ids\n",
        "\n",
        "    # Create data loaders with SubsetRandomSampler\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "    eegnet_model = EEGNet().to(device)\n",
        "    eegnet_model.apply(reset_weights) # Resetting model's weights to avoid weight leakage\n",
        "    optimizer = optim.Adam(eegnet_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        eegnet_model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = eegnet_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {(epoch_accuracy*100):.2f}%\")\n",
        "\n",
        "    # Evaluating on test set\n",
        "    eegnet_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = eegnet_model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_accuracy = correct / total\n",
        "    print(f\"Accuracy for fold {fold}: {(test_accuracy*100):.2f}%\")\n",
        "    print(\"--------------------------------\")\n",
        "    results[fold] = test_accuracy\n",
        "\n",
        "# Print fold results\n",
        "print(f\"K-Fold Cross Validation Results for {k_folds} Folds\")\n",
        "print(\"--------------------------------\")\n",
        "avg_accuracy = sum(results.values()) / len(results)\n",
        "for fold, accuracy in results.items():\n",
        "    print(f'Fold ({fold}): {accuracy:.2f} %')\n",
        "print(f'Average Accuracy: {avg_accuracy:.2f} %')\n"
      ],
      "metadata": {
        "id": "T9Psr1fGaHQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_values = list(results.values())\n",
        "mean_accuracy = np.mean(accuracy_values)\n",
        "std_accuracy = np.std(accuracy_values)\n",
        "\n",
        "# Calculate standard error of the mean (SEM)\n",
        "SEM = std_accuracy / np.sqrt(len(accuracy_values))\n",
        "\n",
        "# Calculate error bars (2 * SEM)\n",
        "error = 2 * SEM\n",
        "\n",
        "# Plot mean accuracy with error bars\n",
        "plt.errorbar(1, mean_accuracy, yerr=error, fmt='o', capsize=5)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Mean Accuracy with Error Bars (2 SEM)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h_uC8UZxaKh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}